{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d21bc2-193a-4a26-9699-fc9a816892ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.16.1-cp312-cp312-macosx_10_15_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow)\n",
      "  Downloading h5py-3.11.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-macosx_10_9_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.3.2-cp312-cp312-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (24.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.31.0)\n",
      "Collecting setuptools (from tensorflow)\n",
      "  Downloading setuptools-70.0.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting typing-extensions>=3.6.6 (from tensorflow)\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.16.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.64.0-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.3 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow)\n",
      "  Downloading keras-3.3.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Downloading wheel-0.43.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting rich (from keras>=3.0.0->tensorflow)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow)\n",
      "  Downloading optree-0.11.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Downloading werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.0.0->tensorflow)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.16.1-cp312-cp312-macosx_10_15_x86_64.whl (259.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.7/259.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.64.0-cp312-cp312-macosx_10_9_universal2.whl (10.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.11.0-cp312-cp312-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-macosx_10_9_x86_64.whl (26.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp312-cp312-macosx_10_9_universal2.whl (393 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.6/393.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-70.0.0-py3-none-any.whl (863 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m863.4/863.4 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Downloading wrapt-1.16.0-cp312-cp312-macosx_10_9_x86_64.whl (37 kB)\n",
      "Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.11.0-cp312-cp312-macosx_10_9_x86_64.whl (300 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.8/300.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, werkzeug, typing-extensions, termcolor, tensorboard-data-server, setuptools, protobuf, opt-einsum, ml-dtypes, mdurl, markdown, h5py, grpcio, google-pasta, gast, absl-py, tensorboard, optree, markdown-it-py, astunparse, rich, keras, tensorflow\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5ad8c3-6844-43e6-8c77-cd7fc9dc9a89",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import ks_2samp, chi2_contingency\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b32e8d3-cf4c-4edc-b3f1-091b9c2e09c4",
   "metadata": {},
   "source": [
    "Create TableGan Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a45784b0-f839-4727-94af-eb761baa63dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"com_salary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "662ded7f-7c35-419c-a631-f381ea868c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 909 entries, 0 to 908\n",
      "Data columns (total 23 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Name              909 non-null    object \n",
      " 1   OttoneuID         909 non-null    int64  \n",
      " 2   FG MajorLeagueID  851 non-null    float64\n",
      " 3   FG MinorLeagueID  902 non-null    object \n",
      " 4   MLB Org           896 non-null    object \n",
      " 5   Position(s)       909 non-null    object \n",
      " 6   Avg Salary        909 non-null    float64\n",
      " 7   Median Salary     909 non-null    float64\n",
      " 8   Min Salary        909 non-null    int64  \n",
      " 9   Max Salary        909 non-null    int64  \n",
      " 10  Last 10           909 non-null    float64\n",
      " 11  Roster%           909 non-null    float64\n",
      " 12  Team              853 non-null    object \n",
      " 13  POS               909 non-null    object \n",
      " 14  ADP               909 non-null    float64\n",
      " 15  rPTS              909 non-null    float64\n",
      " 16  PTS               909 non-null    float64\n",
      " 17  aPOS              909 non-null    float64\n",
      " 18  Dollars           909 non-null    float64\n",
      " 19  Adjusted          909 non-null    float64\n",
      " 20  Cost              262 non-null    float64\n",
      " 21  PlayerId          909 non-null    object \n",
      " 22  value             909 non-null    float64\n",
      "dtypes: float64(13), int64(3), object(7)\n",
      "memory usage: 163.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad8cef89-289b-403d-9eb3-ed57c03733a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>OttoneuID</th>\n",
       "      <th>FG MajorLeagueID</th>\n",
       "      <th>FG MinorLeagueID</th>\n",
       "      <th>MLB Org</th>\n",
       "      <th>Position(s)</th>\n",
       "      <th>Avg Salary</th>\n",
       "      <th>Median Salary</th>\n",
       "      <th>Min Salary</th>\n",
       "      <th>Max Salary</th>\n",
       "      <th>...</th>\n",
       "      <th>POS</th>\n",
       "      <th>ADP</th>\n",
       "      <th>rPTS</th>\n",
       "      <th>PTS</th>\n",
       "      <th>aPOS</th>\n",
       "      <th>Dollars</th>\n",
       "      <th>Adjusted</th>\n",
       "      <th>Cost</th>\n",
       "      <th>PlayerId</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Juan Soto</td>\n",
       "      <td>23717</td>\n",
       "      <td>20123.0</td>\n",
       "      <td>sa906282</td>\n",
       "      <td>NYY</td>\n",
       "      <td>OF</td>\n",
       "      <td>60.31</td>\n",
       "      <td>60.0</td>\n",
       "      <td>34</td>\n",
       "      <td>84</td>\n",
       "      <td>...</td>\n",
       "      <td>OF/DH</td>\n",
       "      <td>10.57</td>\n",
       "      <td>1132.535865</td>\n",
       "      <td>53.429347</td>\n",
       "      <td>21.885623</td>\n",
       "      <td>76.314969</td>\n",
       "      <td>112.280793</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20123</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mookie Betts</td>\n",
       "      <td>18276</td>\n",
       "      <td>13611.0</td>\n",
       "      <td>sa597889</td>\n",
       "      <td>LAD</td>\n",
       "      <td>2B/SS/OF</td>\n",
       "      <td>58.35</td>\n",
       "      <td>58.0</td>\n",
       "      <td>38</td>\n",
       "      <td>87</td>\n",
       "      <td>...</td>\n",
       "      <td>2B/SS/OF</td>\n",
       "      <td>5.03</td>\n",
       "      <td>1036.172159</td>\n",
       "      <td>40.095221</td>\n",
       "      <td>14.934803</td>\n",
       "      <td>56.030024</td>\n",
       "      <td>82.308998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13611</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shohei Ohtani</td>\n",
       "      <td>33600</td>\n",
       "      <td>19755.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LAD</td>\n",
       "      <td>Util/SP</td>\n",
       "      <td>56.13</td>\n",
       "      <td>56.0</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>...</td>\n",
       "      <td>P/DH</td>\n",
       "      <td>12.73</td>\n",
       "      <td>1038.276653</td>\n",
       "      <td>40.386426</td>\n",
       "      <td>12.178202</td>\n",
       "      <td>53.564628</td>\n",
       "      <td>75.049219</td>\n",
       "      <td>54.0</td>\n",
       "      <td>19755</td>\n",
       "      <td>-0.435372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaron Judge</td>\n",
       "      <td>18312</td>\n",
       "      <td>15640.0</td>\n",
       "      <td>sa549847</td>\n",
       "      <td>NYY</td>\n",
       "      <td>OF</td>\n",
       "      <td>54.17</td>\n",
       "      <td>54.5</td>\n",
       "      <td>33</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>OF/DH</td>\n",
       "      <td>11.16</td>\n",
       "      <td>1066.255227</td>\n",
       "      <td>44.257902</td>\n",
       "      <td>21.885623</td>\n",
       "      <td>67.143525</td>\n",
       "      <td>93.070498</td>\n",
       "      <td>50.0</td>\n",
       "      <td>15640</td>\n",
       "      <td>17.143525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Freddie Freeman</td>\n",
       "      <td>5680</td>\n",
       "      <td>5361.0</td>\n",
       "      <td>sa390291</td>\n",
       "      <td>LAD</td>\n",
       "      <td>1B</td>\n",
       "      <td>50.76</td>\n",
       "      <td>51.0</td>\n",
       "      <td>27</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>1B</td>\n",
       "      <td>8.81</td>\n",
       "      <td>1062.235872</td>\n",
       "      <td>43.701733</td>\n",
       "      <td>13.809086</td>\n",
       "      <td>58.510819</td>\n",
       "      <td>81.663561</td>\n",
       "      <td>47.0</td>\n",
       "      <td>5361</td>\n",
       "      <td>11.510819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name  OttoneuID  FG MajorLeagueID FG MinorLeagueID MLB Org  \\\n",
       "0        Juan Soto      23717           20123.0         sa906282     NYY   \n",
       "1     Mookie Betts      18276           13611.0         sa597889     LAD   \n",
       "2    Shohei Ohtani      33600           19755.0              NaN     LAD   \n",
       "3      Aaron Judge      18312           15640.0         sa549847     NYY   \n",
       "4  Freddie Freeman       5680            5361.0         sa390291     LAD   \n",
       "\n",
       "  Position(s)  Avg Salary  Median Salary  Min Salary  Max Salary  ...  \\\n",
       "0          OF       60.31           60.0          34          84  ...   \n",
       "1    2B/SS/OF       58.35           58.0          38          87  ...   \n",
       "2     Util/SP       56.13           56.0          32          88  ...   \n",
       "3          OF       54.17           54.5          33          70  ...   \n",
       "4          1B       50.76           51.0          27          74  ...   \n",
       "\n",
       "        POS    ADP         rPTS        PTS       aPOS    Dollars    Adjusted  \\\n",
       "0     OF/DH  10.57  1132.535865  53.429347  21.885623  76.314969  112.280793   \n",
       "1  2B/SS/OF   5.03  1036.172159  40.095221  14.934803  56.030024   82.308998   \n",
       "2      P/DH  12.73  1038.276653  40.386426  12.178202  53.564628   75.049219   \n",
       "3     OF/DH  11.16  1066.255227  44.257902  21.885623  67.143525   93.070498   \n",
       "4        1B   8.81  1062.235872  43.701733  13.809086  58.510819   81.663561   \n",
       "\n",
       "   Cost  PlayerId      value  \n",
       "0   NaN     20123   0.000000  \n",
       "1   NaN     13611   0.000000  \n",
       "2  54.0     19755  -0.435372  \n",
       "3  50.0     15640  17.143525  \n",
       "4  47.0      5361  11.510819  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d17be43a-b453-4a58-90d8-a224efc68247",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb15b239-2d20-42c4-a228-5a03d3ebe4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of features\n",
    "number_of_features = orig_df.shape[1]  # Assumes no target variable included; adjust if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba93ca5-4c1e-42af-bf14-b1d12978bb2e",
   "metadata": {},
   "source": [
    "## Step 1: Define the Generator\n",
    "The Generator's role in a GAN is to create synthetic data that is indistinguishable from real data. It learns to do this through the adversarial process with the Discriminator. Here, we define a simple neural network model for the\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4620570e-9084-486c-ae8f-a0b4f5ffb140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator\n",
    "class Generator(Model):\n",
    "    def __init__(self, z_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.dense1 = Dense(128, activation='relu')\n",
    "        self.out = Dense(output_dim, activation='tanh')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        return self.out(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664d9d64-07f3-4dc7-b58f-929363d6c489",
   "metadata": {},
   "source": [
    "## Step 2: Define the Discriminator\n",
    "The Discriminator acts as a classifier that tries to distinguish real data from fake data produced by the Generator. This class is also defined with a simple architecture, consisting of one hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "debf91b7-c7b6-4036-a5d8-7af3f12b6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Discriminator\n",
    "class Discriminator(Model):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dense1 = Dense(64, activation='relu')\n",
    "        self.out = Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62193b81-1656-49a9-9d0c-9a2d22122998",
   "metadata": {},
   "source": [
    "## Step 3: Define the TableGAN Model\n",
    "Here we integrate the Generator and Discriminator into a complete GAN model. The TableGAN class manages the training loop where both models are trained in an adversarial setup. This structure includes methods for compiling the model and defining the training step, utilizing TensorFlow's capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "288bde09-72c2-49f9-b382-1c39c880beb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/4.1.4/libexec/lib/python3.12/site-packages/keras/src/backend/tensorflow/nn.py:695: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gen_loss': <tf.Tensor: shape=(), dtype=float32, numpy=0.9087286>,\n",
       " 'disc_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.6692452>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the TableGAN model\n",
    "class TableGAN(Model):\n",
    "    def __init__(self, input_dim, z_dim):\n",
    "        super(TableGAN, self).__init__()\n",
    "        self.generator = Generator(z_dim, input_dim)\n",
    "        self.discriminator = Discriminator(input_dim)\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def compile(self, g_optimizer, d_optimizer, loss_function):\n",
    "        super(TableGAN, self).compile()\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "    def train_step(self, real_data):\n",
    "        batch_size = tf.shape(real_data)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.z_dim))\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            generated_data = self.generator(random_latent_vectors)\n",
    "            real_output = self.discriminator(real_data)\n",
    "            fake_output = self.discriminator(generated_data)\n",
    "            gen_loss = self.loss_function(tf.ones_like(fake_output), fake_output)\n",
    "            real_loss = self.loss_function(tf.ones_like(real_output), real_output)\n",
    "            fake_loss = self.loss_function(tf.zeros_like(fake_output), fake_output)\n",
    "            disc_loss = real_loss + fake_loss\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
    "        self.d_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
    "        return {'gen_loss': gen_loss, 'disc_loss': disc_loss}\n",
    "\n",
    "# Setup and initialization\n",
    "input_dim = orig_df.shape[1]  # Number of features from the original DataFrame\n",
    "z_dim = 100  # Latent dimension for the generator\n",
    "table_gan = TableGAN(input_dim=input_dim, z_dim=z_dim)\n",
    "table_gan.compile(\n",
    "    g_optimizer=Adam(1e-4),\n",
    "    d_optimizer=Adam(1e-4),\n",
    "    loss_function=BinaryCrossentropy(from_logits=True)\n",
    ")\n",
    "\n",
    "# Example use case: simulate real data and train\n",
    "real_data = np.random.normal(size=(10, input_dim))  # Simulate real data\n",
    "table_gan.train_step(real_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2968ae77-bfbd-4a5c-97dc-a3318f073fc6",
   "metadata": {},
   "source": [
    "## Step 4: Data Loading and Preprocessing\n",
    "Before training the model, it's necessary to load and preprocess your data. This step involves reading a CSV file, detecting numerical and categorical columns, and applying appropriate transformations such as scaling for numerical data and one-hot encoding for categorical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a69de58-72c7-4a93-add0-ccba17eb1615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "data = orig_df  # Ensure orig_df is your DataFrame loaded previously\n",
    "\n",
    "# Selecting numerical and categorical columns\n",
    "numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Define preprocessing for numerical columns: impute missing values then scale\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define preprocessing for categorical columns: impute missing values then encode\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Create a ColumnTransformer to apply the above transformations appropriately\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipeline, numerical_cols),\n",
    "        ('cat', cat_pipeline, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Apply transformations to your data\n",
    "processed_data = preprocessor.fit_transform(data)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab2ccd-6c26-4be7-b592-71d5b7aa08b6",
   "metadata": {},
   "source": [
    "## Step 5: Initialize and Train the TableGAN Model\n",
    "Finally, initialize the TableGAN model with dimensions derived from the preprocessed data, compile it with chosen optimizers and loss function, and train the model. Here, we specify the learning rates, the loss function, and the training parameters such as the number of epochs and batch size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "756bde43-54e5-4d56-9196-0f64e19f2eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate synthetic data\n",
    "def generate_synthetic_data(model, num_samples, z_dim):\n",
    "    random_latent_vectors = tf.random.normal(shape=(num_samples, z_dim))\n",
    "    synthetic_data = model.generator(random_latent_vectors).numpy()\n",
    "    return synthetic_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "094e1296-3e51-4ac9-bb41-5720c41e2891",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (177836967.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[20], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    synthetic_data, elapsed_time = generate_synthetic_data(model = TableGAN, num_samples, z_dim = 100)\u001b[0m\n\u001b[0m                                                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "# User input for the number of synthetic samples\n",
    "num_samples = 1000  # Define number of synthetic examples you want\n",
    "\n",
    "# Generate synthetic data\n",
    "synthetic_data = generate_synthetic_data(model=table_gan, num_samples=num_samples, z_dim=100)\n",
    "\n",
    "# Convert the synthetic data to a pandas DataFrame with original column names\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=original_df.columns)\n",
    "print(\"Generated synthetic data shape:\", synthetic_data.shape)\n",
    "print(\"Synthetic DataFrame head:\", synthetic_df.head())\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Shape of synthetic DataFrame:\", synthetic_df.shape)\n",
    "print(synthetic_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb608f2-4c15-4de1-bec2-9e79f828b950",
   "metadata": {},
   "source": [
    "## Step 5: Save to CSV\n",
    "Save the synthetic data to a CSV file. This file can be used for further analysis, sharing, or as input to other analytical tools and systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f892469-3057-4eed-b829-3fe14943d6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to CSV\n",
    "synthetic_df.to_csv('synthetic_data.csv', index=False)\n",
    "\n",
    "print(\"Synthetic data written to synthetic_data.csv successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4cf48c-f359-4595-a8f0-7883a22a8f45",
   "metadata": {},
   "source": [
    "Step 5 Load Datasets: Original and Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de03f85-eac5-4fa9-b328-24319326e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Orig_data = orig_df\n",
    "synthetic_data = synthetic_df\n",
    "\n",
    "\n",
    "print(\"Original Data Columns:\", Orig_data.columns.tolist())\n",
    "print(\"Synthetic Data Columns:\", synthetic_data.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cadc98-9ca7-44fd-b870-8dce49a34c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Define Evaluation Functions\n",
    "# Evaluation metrics implementation\n",
    "\n",
    "# Fidelity Metrics\n",
    "# Fidelity Metrics\n",
    "def evaluate_fidelity(real_data, synthetic_data, continuous_columns, categorical_columns):\n",
    "    ks_results = {col: ks_2samp(real_data[col], synthetic_data[col]).statistic for col in continuous_columns}\n",
    "    chi_squared_results = {col: chi2_contingency(pd.crosstab(real_data[col], synthetic_data[col]))[:2] for col in categorical_columns}\n",
    "    return {'KS Test': ks_results, 'Chi-Squared Test': chi_squared_results}\n",
    "\n",
    "# Utility Metrics\n",
    "def evaluate_predictive_performance(real_data, synthetic_data, target_column, test_size=0.3, random_state=42):\n",
    "    X_real = real_data.drop(columns=[target_column])\n",
    "    y_real = real_data[target_column]\n",
    "    X_synthetic = synthetic_data.drop(columns=[target_column])\n",
    "    y_synthetic = synthetic_data[target_column]\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "    model.fit(X_synthetic, y_synthetic)\n",
    "    predictions = model.predict(X_real)\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_real, predictions),\n",
    "        'ROC AUC': roc_auc_score(y_real, model.predict_proba(X_real)[:, 1]),\n",
    "        'F1 Score': f1_score(y_real, predictions)\n",
    "    }\n",
    "\n",
    "def informativeness_test(real_data, synthetic_data, test_size=0.3, random_state=42):\n",
    "    real_data['is_real'] = 1\n",
    "    synthetic_data['is_real'] = 0\n",
    "    combined_data = pd.concat([real_data, synthetic_data], ignore_index=True)\n",
    "    X = combined_data.drop(columns=['is_real'])\n",
    "    y = combined_data['is_real']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    classifier = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    predictions = classifier.predict(X_test)\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_test, predictions),\n",
    "        'ROC AUC': roc_auc_score(y_test, classifier.predict_proba(X_test)[:, 1]),\n",
    "        'F1 Score': f1_score(y_test, predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8006a-8fd2-4f7b-814d-bb592c77aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns to apply metrics\n",
    "continuous_columns = original_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_columns = original_df.select_dtypes(include=['object']).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28743071-1e4d-45b4-9aa6-bb465485ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate fidelity\n",
    "fidelity_results = evaluate_fidelity(original_df, synthetic_df, continuous_columns, categorical_columns)\n",
    "print(\"Fidelity Results:\", fidelity_results)\n",
    "\n",
    "# Assuming the target column is correctly named and exists in your DataFrame\n",
    "predictive_performance_results = evaluate_predictive_performance(original_df, synthetic_df, 'target_column')\n",
    "print(\"Predictive Performance Results:\", predictive_performance_results)\n",
    "\n",
    "# Informativeness test\n",
    "informativeness_results = informativeness_test(original_df, synthetic_df)\n",
    "print(\"Informativeness Results:\", informativeness_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a486699-5079-41c5-a7e0-a97f41d38792",
   "metadata": {},
   "source": [
    "# Evaluate predictive performance\n",
    "predictive_performance_results = evaluate_predictive_performance(real_data, synthetic_data, 'target_column')\n",
    "print(\"Predictive Performance Results:\", predictive_performance_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
