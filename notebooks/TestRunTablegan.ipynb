{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5ad8c3-6844-43e6-8c77-cd7fc9dc9a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 15:56:33.985306: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b32e8d3-cf4c-4edc-b3f1-091b9c2e09c4",
   "metadata": {},
   "source": [
    "Create TableGan Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45784b0-f839-4727-94af-eb761baa63dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"com_salary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ded7f-7c35-419c-a631-f381ea868c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8cef89-289b-403d-9eb3-ed57c03733a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17be43a-b453-4a58-90d8-a224efc68247",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df= df[['ADP', 'Avg Salary', 'Median Salary', 'Min Salary', 'Max Salary', 'rPTS', 'PTS', 'Dollars']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb15b239-2d20-42c4-a228-5a03d3ebe4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba93ca5-4c1e-42af-bf14-b1d12978bb2e",
   "metadata": {},
   "source": [
    "## Step 1: Define the Generator\n",
    "The Generator's role in a GAN is to create synthetic data that is indistinguishable from real data. It learns to do this through the adversarial process with the Discriminator. Here, we define a simple neural network model for the\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96be28a-87b0-4c1b-a2bb-66bcaa3a762c",
   "metadata": {},
   "source": [
    "# Define the Generator\n",
    "class Generator(Model):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.dense1 = layers.Dense(64, activation='relu')\n",
    "        self.out = layers.Dense(z_dim, activation='tanh')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.dense1(x)\n",
    "        return self.out(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664d9d64-07f3-4dc7-b58f-929363d6c489",
   "metadata": {},
   "source": [
    "## Step 2: Define the Discriminator\n",
    "The Discriminator acts as a classifier that tries to distinguish real data from fake data produced by the Generator. This class is also defined with a simple architecture, consisting of one hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf91b7-c7b6-4036-a5d8-7af3f12b6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Discriminator\n",
    "class Discriminator(Model):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dense1 = layers.Dense(64, activation='relu')\n",
    "        self.out = layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.dense1(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62193b81-1656-49a9-9d0c-9a2d22122998",
   "metadata": {},
   "source": [
    "## Step 3: Define the TableGAN Model\n",
    "Here we integrate the Generator and Discriminator into a complete GAN model. The TableGAN class manages the training loop where both models are trained in an adversarial setup. This structure includes methods for compiling the model and defining the training step, utilizing TensorFlow's capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194c6558-0dac-46a5-9569-4d7ba69721cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TableGAN model\n",
    "class TableGAN(Model):\n",
    "    def __init__(self, input_dim, z_dim):\n",
    "        super(TableGAN, self).__init__()\n",
    "        self.generator = Generator(z_dim)\n",
    "        self.discriminator = Discriminator(input_dim)\n",
    "\n",
    "    def compile(self, g_optimizer, d_optimizer, loss_function):\n",
    "        super(TableGAN, self).compile()\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "    def train_step(self, real_data):\n",
    "        batch_size = tf.shape(real_data)[0]\n",
    "        z_dim = self.generator.layers[-1].output_shape[-1]\n",
    "\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, z_dim))\n",
    "\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            generated_data = self.generator(random_latent_vectors)\n",
    "            real_output = self.discriminator(real_data)\n",
    "            fake_output = self.discriminator(generated_data)\n",
    "\n",
    "            gen_loss = self.loss_function(tf.ones_like(fake_output), fake_output)\n",
    "            real_loss = self.loss_function(tf.ones_like(real_output), real_output)\n",
    "            fake_loss = self.loss_function(tf.zeros_like(fake_output), fake_output)\n",
    "            disc_loss = real_loss + fake_loss\n",
    "\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "\n",
    "        self.g_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
    "        self.d_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
    "\n",
    "        return {'gen_loss': gen_loss, 'disc_loss': disc_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2968ae77-bfbd-4a5c-97dc-a3318f073fc6",
   "metadata": {},
   "source": [
    "## Step 4: Data Loading and Preprocessing\n",
    "Before training the model, it's necessary to load and preprocess your data. This step involves reading a CSV file, detecting numerical and categorical columns, and applying appropriate transformations such as scaling for numerical data and one-hot encoding for categorical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a69de58-72c7-4a93-add0-ccba17eb1615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "data = pd.read_csv('path_to_your_dataset.csv')\n",
    "numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(), categorical_cols)\n",
    "    ])\n",
    "processed_data = preprocessor.fit_transform(data)\n",
    "\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab2ccd-6c26-4be7-b592-71d5b7aa08b6",
   "metadata": {},
   "source": [
    "## Step 5: Initialize and Train the TableGAN Model\n",
    "Finally, initialize the TableGAN model with dimensions derived from the preprocessed data, compile it with chosen optimizers and loss function, and train the model. Here, we specify the learning rates, the loss function, and the training parameters such as the number of epochs and batch size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "756bde43-54e5-4d56-9196-0f64e19f2eeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2227514722.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    loss_function=BinaryCrossentropy(from_logits=True)\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and compile\n",
    "input_dim = processed_data.shape[1]\n",
    "z_dim = 50\n",
    "tablegan = TableGAN(input_dim=input_dim, z_dim=z_dim)\n",
    "tablegan.compile(\n",
    "    g_optimizer=Adam(1e-4),\n",
    "    d_optimizer=Adam(1e-4),\n",
    "    loss_function=BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "tablegan.fit(processed_data, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb1858-2ba0-46c7-bdd6-4bd21d20abb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
