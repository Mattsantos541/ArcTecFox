{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cda62c9-713a-4ccf-b095-20066c5c9ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables set. Please restart the Jupyter kernel now.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables to control OpenMP and MKL\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "os.environ['KMP_INIT_AT_FORK'] = 'FALSE'\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "\n",
    "# Print to confirm setting environment variables\n",
    "print(\"Environment variables set. Please restart the Jupyter kernel now.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "818fd9ab-9a1c-4c9a-8ee1-88f08e83a838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: ['OttoneuID', 'FG MajorLeagueID', 'Avg Salary', 'Median Salary', 'Min Salary', 'Max Salary', 'Last 10', 'Roster%', 'ADP', 'rPTS', 'PTS', 'aPOS', 'Dollars', 'Adjusted', 'Cost', 'value']\n",
      "Categorical features: ['Name', 'FG MinorLeagueID', 'MLB Org', 'Position(s)', 'Team', 'POS', 'PlayerId']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ctgan import CTGAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import ks_2samp, chi2_contingency\n",
    "import time\n",
    "\n",
    "# Load the original dataset\n",
    "original_df = pd.read_csv('com_salary.csv')  # Update with the actual path to your dataset\n",
    "\n",
    "# Selecting numeric and categorical features\n",
    "numeric_features = original_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = original_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa37ecf2-3e81-4f92-9e25-625a28ccafae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of prepared_data: (909, 2901)\n",
      "Number of columns in prepared_data: 2901\n",
      "Number of column names: 2901\n",
      "Data preprocessing completed in 0.11 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Define transformers\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers into a preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "start_time = time.time()\n",
    "prepared_data = preprocessor.fit_transform(original_df)\n",
    "end_time = time.time()\n",
    "preprocessing_time = end_time - start_time\n",
    "\n",
    "# Check if the data is sparse and convert to dense if necessary\n",
    "if hasattr(prepared_data, \"toarray\"):\n",
    "    prepared_data = prepared_data.toarray()\n",
    "\n",
    "# Check the shape of the prepared data\n",
    "print(f\"Shape of prepared_data: {prepared_data.shape}\")\n",
    "\n",
    "# Get the column names from the transformers\n",
    "numeric_column_names = numeric_features\n",
    "categorical_column_names = preprocessor.named_transformers_['cat']['encoder'].get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combine all column names\n",
    "all_column_names = numeric_column_names + list(categorical_column_names)\n",
    "\n",
    "# Verify the lengths match\n",
    "print(f\"Number of columns in prepared_data: {prepared_data.shape[1]}\")\n",
    "print(f\"Number of column names: {len(all_column_names)}\")\n",
    "\n",
    "# Ensure the lengths match before creating the DataFrame\n",
    "if prepared_data.shape[1] == len(all_column_names):\n",
    "    prepared_df = pd.DataFrame(prepared_data, columns=all_column_names)\n",
    "else:\n",
    "    raise ValueError(\"Mismatch between prepared data columns and column names.\")\n",
    "\n",
    "print(f\"Data preprocessing completed in {preprocessing_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e39a103-7368-4cec-94fb-95540b0eaaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the CTGAN model\n",
    "epochs = 2  # Adjust the number of epochs as needed\n",
    "ctgan = CTGAN(epochs=epochs)\n",
    "\n",
    "# Train the CTGAN model\n",
    "start_time = time.time()\n",
    "#with threadpool_limits(limits=1, user_api='blas'):\n",
    "ctgan.fit(prepared_df, list(categorical_column_names))\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Function to generate synthetic data and measure time taken\n",
    "def generate_synthetic_data(ctgan_model, num_samples, columns):\n",
    "    start_time = time.time()\n",
    "    synthetic_data = ctgan_model.sample(num_samples)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Convert synthetic data to a pandas DataFrame with appropriate column names\n",
    "    synthetic_df = pd.DataFrame(synthetic_data, columns=columns)\n",
    "    \n",
    "    return synthetic_df, elapsed_time\n",
    "\n",
    "# Specify the number of synthetic samples to generate\n",
    "additional_rows = 1000  # Example: add 1000 more rows\n",
    "num_samples = len(original_df) + additional_rows\n",
    "\n",
    "# Generate synthetic data\n",
    "synthetic_df, generation_time = generate_synthetic_data(ctgan, num_samples, all_column_names)\n",
    "\n",
    "# Save the synthetic data to a CSV file\n",
    "output_file = 'synthetic_data_ctgan.csv'\n",
    "synthetic_df.to_csv(output_file, index=False)\n",
    "print(f\"Synthetic data written to '{output_file}' successfully with {num_samples} samples.\")\n",
    "print(f\"Synthetic data generation completed in {generation_time:.2f} seconds.\")\n",
    "print(f\"CTGAN model training completed in {training_time:.2f} seconds.\")\n",
    "\n",
    "# Print some outputs to verify\n",
    "print(\"Generated synthetic data shape:\", synthetic_df.shape)\n",
    "print(\"Sample of synthetic data:\")\n",
    "print(synthetic_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef562e6-0dd2-421d-8f4f-77e3884b589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fidelity(real_data, synthetic_data, continuous_columns, categorical_columns):\n",
    "    ks_results = {col: ks_2samp(real_data[col], synthetic_data[col]).statistic for col in continuous_columns}\n",
    "    chi_squared_results = {col: chi2_contingency(pd.crosstab(real_data[col], synthetic_data[col]))[:2] for col in categorical_columns}\n",
    "    return {'KS Test': ks_results, 'Chi-Squared Test': chi_squared_results}\n",
    "\n",
    "def evaluate_predictive_performance(real_data, synthetic_data, target_column, test_size=0.3, random_state=42):\n",
    "    X_real = real_data.drop(columns=[target_column])\n",
    "    y_real = real_data[target_column]\n",
    "    X_synthetic = synthetic_data.drop(columns=[target_column])\n",
    "    y_synthetic = synthetic_data[target_column]\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "    model.fit(X_synthetic, y_synthetic)\n",
    "    predictions = model.predict(X_real)\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_real, predictions),\n",
    "        'ROC AUC': roc_auc_score(y_real, model.predict_proba(X_real)[:, 1]),\n",
    "        'F1 Score': f1_score(y_real, predictions)\n",
    "    }\n",
    "\n",
    "def informativeness_test(real_data, synthetic_data, test_size=0.3, random_state=42):\n",
    "    real_data['is_real'] = 1\n",
    "    synthetic_data['is_real'] = 0\n",
    "    combined_data = pd.concat([real_data, synthetic_data], ignore_index=True)\n",
    "    X = combined_data.drop(columns=['is_real'])\n",
    "    y = combined_data['is_real']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    classifier = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    predictions = classifier.predict(X_test)\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_test, predictions),\n",
    "        'ROC AUC': roc_auc_score(y_test, classifier.predict_proba(X_test)[:, 1]),\n",
    "        'F1 Score': f1_score(y_test, predictions)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e3f61-d203-4266-912d-ffaa1f7e521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_synthetic_data(original_df, synthetic_df, target_column):\n",
    "    # Identify columns to apply metrics\n",
    "    continuous_columns = original_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_columns = original_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Evaluate fidelity\n",
    "    fidelity_results = evaluate_fidelity(original_df, synthetic_df, continuous_columns, categorical_columns)\n",
    "    print(\"Fidelity Results:\", fidelity_results)\n",
    "\n",
    "    # Evaluate predictive performance\n",
    "    if target_column in original_df.columns:\n",
    "        predictive_performance_results = evaluate_predictive_performance(original_df, synthetic_df, target_column)\n",
    "        print(\"Predictive Performance Results:\", predictive_performance_results)\n",
    "    else:\n",
    "        print(f\"Error: Target column '{target_column}' does not exist in the DataFrame.\")\n",
    "\n",
    "    # Evaluate informativeness\n",
    "    informativeness_results = informativeness_test(original_df, synthetic_df)\n",
    "    print(\"Informativeness Results:\", informativeness_results)\n",
    "\n",
    "# Specify your target column\n",
    "target_column = 'your_target_column'  # Replace with your actual target column name\n",
    "evaluate_synthetic_data(original_df, synthetic_df, target_column)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
