{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5ad8c3-6844-43e6-8c77-cd7fc9dc9a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 15:36:24.554464: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import ks_2samp, chi2_contingency\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b32e8d3-cf4c-4edc-b3f1-091b9c2e09c4",
   "metadata": {},
   "source": [
    "Create TableGan Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a45784b0-f839-4727-94af-eb761baa63dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"com_salary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "662ded7f-7c35-419c-a631-f381ea868c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 909 entries, 0 to 908\n",
      "Data columns (total 23 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Name              909 non-null    object \n",
      " 1   OttoneuID         909 non-null    int64  \n",
      " 2   FG MajorLeagueID  851 non-null    float64\n",
      " 3   FG MinorLeagueID  902 non-null    object \n",
      " 4   MLB Org           896 non-null    object \n",
      " 5   Position(s)       909 non-null    object \n",
      " 6   Avg Salary        909 non-null    float64\n",
      " 7   Median Salary     909 non-null    float64\n",
      " 8   Min Salary        909 non-null    int64  \n",
      " 9   Max Salary        909 non-null    int64  \n",
      " 10  Last 10           909 non-null    float64\n",
      " 11  Roster%           909 non-null    float64\n",
      " 12  Team              853 non-null    object \n",
      " 13  POS               909 non-null    object \n",
      " 14  ADP               909 non-null    float64\n",
      " 15  rPTS              909 non-null    float64\n",
      " 16  PTS               909 non-null    float64\n",
      " 17  aPOS              909 non-null    float64\n",
      " 18  Dollars           909 non-null    float64\n",
      " 19  Adjusted          909 non-null    float64\n",
      " 20  Cost              262 non-null    float64\n",
      " 21  PlayerId          909 non-null    object \n",
      " 22  value             909 non-null    float64\n",
      "dtypes: float64(13), int64(3), object(7)\n",
      "memory usage: 163.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad8cef89-289b-403d-9eb3-ed57c03733a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>OttoneuID</th>\n",
       "      <th>FG MajorLeagueID</th>\n",
       "      <th>FG MinorLeagueID</th>\n",
       "      <th>MLB Org</th>\n",
       "      <th>Position(s)</th>\n",
       "      <th>Avg Salary</th>\n",
       "      <th>Median Salary</th>\n",
       "      <th>Min Salary</th>\n",
       "      <th>Max Salary</th>\n",
       "      <th>...</th>\n",
       "      <th>POS</th>\n",
       "      <th>ADP</th>\n",
       "      <th>rPTS</th>\n",
       "      <th>PTS</th>\n",
       "      <th>aPOS</th>\n",
       "      <th>Dollars</th>\n",
       "      <th>Adjusted</th>\n",
       "      <th>Cost</th>\n",
       "      <th>PlayerId</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Juan Soto</td>\n",
       "      <td>23717</td>\n",
       "      <td>20123.0</td>\n",
       "      <td>sa906282</td>\n",
       "      <td>NYY</td>\n",
       "      <td>OF</td>\n",
       "      <td>60.31</td>\n",
       "      <td>60.0</td>\n",
       "      <td>34</td>\n",
       "      <td>84</td>\n",
       "      <td>...</td>\n",
       "      <td>OF/DH</td>\n",
       "      <td>10.57</td>\n",
       "      <td>1132.535865</td>\n",
       "      <td>53.429347</td>\n",
       "      <td>21.885623</td>\n",
       "      <td>76.314969</td>\n",
       "      <td>112.280793</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20123</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mookie Betts</td>\n",
       "      <td>18276</td>\n",
       "      <td>13611.0</td>\n",
       "      <td>sa597889</td>\n",
       "      <td>LAD</td>\n",
       "      <td>2B/SS/OF</td>\n",
       "      <td>58.35</td>\n",
       "      <td>58.0</td>\n",
       "      <td>38</td>\n",
       "      <td>87</td>\n",
       "      <td>...</td>\n",
       "      <td>2B/SS/OF</td>\n",
       "      <td>5.03</td>\n",
       "      <td>1036.172159</td>\n",
       "      <td>40.095221</td>\n",
       "      <td>14.934803</td>\n",
       "      <td>56.030024</td>\n",
       "      <td>82.308998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13611</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shohei Ohtani</td>\n",
       "      <td>33600</td>\n",
       "      <td>19755.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LAD</td>\n",
       "      <td>Util/SP</td>\n",
       "      <td>56.13</td>\n",
       "      <td>56.0</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>...</td>\n",
       "      <td>P/DH</td>\n",
       "      <td>12.73</td>\n",
       "      <td>1038.276653</td>\n",
       "      <td>40.386426</td>\n",
       "      <td>12.178202</td>\n",
       "      <td>53.564628</td>\n",
       "      <td>75.049219</td>\n",
       "      <td>54.0</td>\n",
       "      <td>19755</td>\n",
       "      <td>-0.435372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaron Judge</td>\n",
       "      <td>18312</td>\n",
       "      <td>15640.0</td>\n",
       "      <td>sa549847</td>\n",
       "      <td>NYY</td>\n",
       "      <td>OF</td>\n",
       "      <td>54.17</td>\n",
       "      <td>54.5</td>\n",
       "      <td>33</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>OF/DH</td>\n",
       "      <td>11.16</td>\n",
       "      <td>1066.255227</td>\n",
       "      <td>44.257902</td>\n",
       "      <td>21.885623</td>\n",
       "      <td>67.143525</td>\n",
       "      <td>93.070498</td>\n",
       "      <td>50.0</td>\n",
       "      <td>15640</td>\n",
       "      <td>17.143525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Freddie Freeman</td>\n",
       "      <td>5680</td>\n",
       "      <td>5361.0</td>\n",
       "      <td>sa390291</td>\n",
       "      <td>LAD</td>\n",
       "      <td>1B</td>\n",
       "      <td>50.76</td>\n",
       "      <td>51.0</td>\n",
       "      <td>27</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>1B</td>\n",
       "      <td>8.81</td>\n",
       "      <td>1062.235872</td>\n",
       "      <td>43.701733</td>\n",
       "      <td>13.809086</td>\n",
       "      <td>58.510819</td>\n",
       "      <td>81.663561</td>\n",
       "      <td>47.0</td>\n",
       "      <td>5361</td>\n",
       "      <td>11.510819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name  OttoneuID  FG MajorLeagueID FG MinorLeagueID MLB Org  \\\n",
       "0        Juan Soto      23717           20123.0         sa906282     NYY   \n",
       "1     Mookie Betts      18276           13611.0         sa597889     LAD   \n",
       "2    Shohei Ohtani      33600           19755.0              NaN     LAD   \n",
       "3      Aaron Judge      18312           15640.0         sa549847     NYY   \n",
       "4  Freddie Freeman       5680            5361.0         sa390291     LAD   \n",
       "\n",
       "  Position(s)  Avg Salary  Median Salary  Min Salary  Max Salary  ...  \\\n",
       "0          OF       60.31           60.0          34          84  ...   \n",
       "1    2B/SS/OF       58.35           58.0          38          87  ...   \n",
       "2     Util/SP       56.13           56.0          32          88  ...   \n",
       "3          OF       54.17           54.5          33          70  ...   \n",
       "4          1B       50.76           51.0          27          74  ...   \n",
       "\n",
       "        POS    ADP         rPTS        PTS       aPOS    Dollars    Adjusted  \\\n",
       "0     OF/DH  10.57  1132.535865  53.429347  21.885623  76.314969  112.280793   \n",
       "1  2B/SS/OF   5.03  1036.172159  40.095221  14.934803  56.030024   82.308998   \n",
       "2      P/DH  12.73  1038.276653  40.386426  12.178202  53.564628   75.049219   \n",
       "3     OF/DH  11.16  1066.255227  44.257902  21.885623  67.143525   93.070498   \n",
       "4        1B   8.81  1062.235872  43.701733  13.809086  58.510819   81.663561   \n",
       "\n",
       "   Cost  PlayerId      value  \n",
       "0   NaN     20123   0.000000  \n",
       "1   NaN     13611   0.000000  \n",
       "2  54.0     19755  -0.435372  \n",
       "3  50.0     15640  17.143525  \n",
       "4  47.0      5361  11.510819  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d17be43a-b453-4a58-90d8-a224efc68247",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb15b239-2d20-42c4-a228-5a03d3ebe4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of features\n",
    "number_of_features = orig_df.shape[1]  # Assumes no target variable included; adjust if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba93ca5-4c1e-42af-bf14-b1d12978bb2e",
   "metadata": {},
   "source": [
    "## Step 1: Define the Generator\n",
    "The Generator's role in a GAN is to create synthetic data that is indistinguishable from real data. It learns to do this through the adversarial process with the Discriminator. Here, we define a simple neural network model for the\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4620570e-9084-486c-ae8f-a0b4f5ffb140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator\n",
    "class Generator(Model):\n",
    "    def __init__(self, z_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.dense1 = Dense(128, activation='relu')\n",
    "        self.out = Dense(output_dim, activation='tanh')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        return self.out(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664d9d64-07f3-4dc7-b58f-929363d6c489",
   "metadata": {},
   "source": [
    "## Step 2: Define the Discriminator\n",
    "The Discriminator acts as a classifier that tries to distinguish real data from fake data produced by the Generator. This class is also defined with a simple architecture, consisting of one hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "debf91b7-c7b6-4036-a5d8-7af3f12b6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Discriminator\n",
    "class Discriminator(Model):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dense1 = Dense(64, activation='relu')\n",
    "        self.out = Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62193b81-1656-49a9-9d0c-9a2d22122998",
   "metadata": {},
   "source": [
    "## Step 3: Define the TableGAN Model\n",
    "Here we integrate the Generator and Discriminator into a complete GAN model. The TableGAN class manages the training loop where both models are trained in an adversarial setup. This structure includes methods for compiling the model and defining the training step, utilizing TensorFlow's capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "288bde09-72c2-49f9-b382-1c39c880beb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/4.1.4/libexec/lib/python3.12/site-packages/keras/src/backend/tensorflow/nn.py:695: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gen_loss': <tf.Tensor: shape=(), dtype=float32, numpy=0.73563075>,\n",
       " 'disc_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.4861012>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the TableGAN model\n",
    "class TableGAN(Model):\n",
    "    def __init__(self, input_dim, z_dim):\n",
    "        super(TableGAN, self).__init__()\n",
    "        self.generator = Generator(z_dim, input_dim)\n",
    "        self.discriminator = Discriminator(input_dim)\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def compile(self, g_optimizer, d_optimizer, loss_function):\n",
    "        super(TableGAN, self).compile()\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "    def train_step(self, real_data):\n",
    "        batch_size = tf.shape(real_data)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.z_dim))\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            generated_data = self.generator(random_latent_vectors)\n",
    "            real_output = self.discriminator(real_data)\n",
    "            fake_output = self.discriminator(generated_data)\n",
    "            gen_loss = self.loss_function(tf.ones_like(fake_output), fake_output)\n",
    "            real_loss = self.loss_function(tf.ones_like(real_output), real_output)\n",
    "            fake_loss = self.loss_function(tf.zeros_like(fake_output), fake_output)\n",
    "            disc_loss = real_loss + fake_loss\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
    "        self.d_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
    "        return {'gen_loss': gen_loss, 'disc_loss': disc_loss}\n",
    "\n",
    "# Setup and initialization\n",
    "input_dim = original_df.shape[1]  # Number of features from the original DataFrame\n",
    "z_dim = 100  # Latent dimension for the generator\n",
    "table_gan = TableGAN(input_dim=input_dim, z_dim=z_dim)\n",
    "table_gan.compile(\n",
    "    g_optimizer=Adam(1e-4),\n",
    "    d_optimizer=Adam(1e-4),\n",
    "    loss_function=BinaryCrossentropy(from_logits=True)\n",
    ")\n",
    "\n",
    "# Example use case: simulate real data and train\n",
    "real_data = np.random.normal(size=(10, input_dim))  # Simulate real data\n",
    "table_gan.train_step(real_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2968ae77-bfbd-4a5c-97dc-a3318f073fc6",
   "metadata": {},
   "source": [
    "## Step 4: Data Loading and Preprocessing\n",
    "Before training the model, it's necessary to load and preprocess your data. This step involves reading a CSV file, detecting numerical and categorical columns, and applying appropriate transformations such as scaling for numerical data and one-hot encoding for categorical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a69de58-72c7-4a93-add0-ccba17eb1615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "data = orig_df  # Ensure orig_df is your DataFrame loaded previously\n",
    "\n",
    "# Selecting numerical and categorical columns\n",
    "numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Define preprocessing for numerical columns: impute missing values then scale\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define preprocessing for categorical columns: impute missing values then encode\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Create a ColumnTransformer to apply the above transformations appropriately\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipeline, numerical_cols),\n",
    "        ('cat', cat_pipeline, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Apply transformations to your data\n",
    "processed_data = preprocessor.fit_transform(data)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab2ccd-6c26-4be7-b592-71d5b7aa08b6",
   "metadata": {},
   "source": [
    "## Step 5: Initialize and Train the TableGAN Model\n",
    "Finally, initialize the TableGAN model with dimensions derived from the preprocessed data, compile it with chosen optimizers and loss function, and train the model. Here, we specify the learning rates, the loss function, and the training parameters such as the number of epochs and batch size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "756bde43-54e5-4d56-9196-0f64e19f2eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate synthetic data\n",
    "def generate_synthetic_data(num_samples, z_dim):\n",
    "    random_latent_vectors = tf.random.normal(shape=(num_samples, z_dim))\n",
    "    start_time = time.time()  # Start timing\n",
    "    synthetic_data = table_gan.generator(random_latent_vectors).numpy()\n",
    "    end_time = time.time()  # End timing\n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "    return synthetic_data, elapsed_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "094e1296-3e51-4ac9-bb41-5720c41e2891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of synthetic data samples to generate:  10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data generation completed in 0.01 seconds.\n",
      "Shape of synthetic DataFrame: (10000, 8)\n",
      "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
      "0  -0.698681   0.450539   0.410327   0.826872  -0.914449   0.212824   \n",
      "1  -0.042047   0.673032  -0.318805   0.952034  -0.560653  -0.089425   \n",
      "2   0.221194   0.462863   0.383772   0.603048  -0.923383   0.155692   \n",
      "3   0.888527   0.331412  -0.281313   0.298879  -0.231854   0.265547   \n",
      "4  -0.078003   0.907564  -0.631172   0.433976  -0.876646  -0.395648   \n",
      "\n",
      "   feature_7  feature_8  \n",
      "0  -0.878574   0.435100  \n",
      "1  -0.846840   0.044775  \n",
      "2   0.004034   0.603045  \n",
      "3  -0.192156   0.251017  \n",
      "4  -0.675198  -0.008606  \n"
     ]
    }
   ],
   "source": [
    "# User input for the number of synthetic samples\n",
    "num_samples = int(input(\"Enter the number of synthetic data samples to generate: \"))\n",
    "\n",
    "# Generate synthetic data and record timing\n",
    "synthetic_data, elapsed_time = generate_synthetic_data(num_samples, z_dim)\n",
    "\n",
    "# Convert the synthetic data to a pandas DataFrame\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=[f\"feature_{i+1}\" for i in range(synthetic_data.shape[1])])\n",
    "\n",
    "# Display the timing information\n",
    "print(f\"Synthetic data generation completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Shape of synthetic DataFrame:\", synthetic_df.shape)\n",
    "print(synthetic_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb608f2-4c15-4de1-bec2-9e79f828b950",
   "metadata": {},
   "source": [
    "## Step 5: Save to CSV\n",
    "Save the synthetic data to a CSV file. This file can be used for further analysis, sharing, or as input to other analytical tools and systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f892469-3057-4eed-b829-3fe14943d6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data written to synthetic_data.csv successfully!\n"
     ]
    }
   ],
   "source": [
    "# Write to CSV\n",
    "synthetic_df.to_csv('synthetic_data.csv', index=False)\n",
    "\n",
    "print(\"Synthetic data written to synthetic_data.csv successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4cf48c-f359-4595-a8f0-7883a22a8f45",
   "metadata": {},
   "source": [
    "Step 5 Load Datasets: Original and Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5de03f85-eac5-4fa9-b328-24319326e2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Columns: ['ADP', 'Avg Salary', 'Median Salary', 'Min Salary', 'Max Salary', 'rPTS', 'PTS', 'Dollars']\n",
      "Synthetic Data Columns: ['feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8']\n"
     ]
    }
   ],
   "source": [
    "Orig_data = orig_df\n",
    "synthetic_data = synthetic_df\n",
    "\n",
    "\n",
    "print(\"Original Data Columns:\", Orig_data.columns.tolist())\n",
    "print(\"Synthetic Data Columns:\", synthetic_data.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62cadc98-9ca7-44fd-b870-8dce49a34c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Define Evaluation Functions\n",
    "# Evaluation metrics implementation\n",
    "\n",
    "# Fidelity Metrics\n",
    "def evaluate_fidelity(Orig_data, synthetic_data, continuous_columns, categorical_columns):\n",
    "    ks_results = {col: ks_2samp(Orig_data[col], synthetic_data[col]) for col in continuous_columns}\n",
    "    chi_squared_results = {col: chi2_contingency(pd.crosstab(Orig_data[col], synthetic_data[col]))[:2] for col in categorical_columns}\n",
    "    return {'KS Test': ks_results, 'Chi-Squared Test': chi_squared_results}\n",
    "\n",
    "# Utility Metrics\n",
    "def evaluate_predictive_performance(Orig_data, synthetic_data, target_column, test_size=0.3, random_state=42):\n",
    "    X_real = Orig_data.drop(columns=[target_column])\n",
    "    y_real = Orig_data[target_column]\n",
    "    X_train_real, X_test_real, y_train_real, y_test_real = train_test_split(X_real, y_real, test_size=test_size, random_state=random_state)\n",
    "    X_synthetic = synthetic_data.drop(columns=[target_column])\n",
    "    y_synthetic = synthetic_data[target_column]\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "    model.fit(X_synthetic, y_synthetic)\n",
    "    predictions = model.predict(X_test_real)\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_test_real, predictions),\n",
    "        'ROC AUC': roc_auc_score(y_test_real, model.predict_proba(X_test_real)[:, 1]),\n",
    "        'F1 Score': f1_score(y_test_real, predictions)\n",
    "    }\n",
    "\n",
    "def informativeness_test(Orig_data, synthetic_data, test_size=0.3, random_state=42):\n",
    "    orig_data['is_real'] = 1\n",
    "    synthetic_data['is_real'] = 0\n",
    "    combined_data = pd.concat([real_data, synthetic_data], ignore_index=True)\n",
    "    X = combined_data.drop(columns=['is_real'])\n",
    "    y = combined_data['is_real']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    classifier = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    predictions = classifier.predict(X_test)\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_test, predictions),\n",
    "        'ROC AUC': roc_auc_score(y_test, classifier.predict_proba(X_test)[:, 1]),\n",
    "        'F1 Score': f1_score(y_test, predictions)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28743071-1e4d-45b4-9aa6-bb465485ae60",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ADP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/4.1.4/libexec/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ADP'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m categorical_columns \u001b[38;5;241m=\u001b[39m Orig_data\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Evaluate fidelity\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m fidelity_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_fidelity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOrig_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynthetic_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontinuous_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFidelity Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m, fidelity_results)\n",
      "Cell \u001b[0;32mIn[30], line 6\u001b[0m, in \u001b[0;36mevaluate_fidelity\u001b[0;34m(Orig_data, synthetic_data, continuous_columns, categorical_columns)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_fidelity\u001b[39m(Orig_data, synthetic_data, continuous_columns, categorical_columns):\n\u001b[0;32m----> 6\u001b[0m     ks_results \u001b[38;5;241m=\u001b[39m {col: ks_2samp(Orig_data[col], \u001b[43msynthetic_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m continuous_columns}\n\u001b[1;32m      7\u001b[0m     chi_squared_results \u001b[38;5;241m=\u001b[39m {col: chi2_contingency(pd\u001b[38;5;241m.\u001b[39mcrosstab(Orig_data[col], synthetic_data[col]))[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m categorical_columns}\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKS Test\u001b[39m\u001b[38;5;124m'\u001b[39m: ks_results, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChi-Squared Test\u001b[39m\u001b[38;5;124m'\u001b[39m: chi_squared_results}\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/4.1.4/libexec/lib/python3.12/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/usr/local/Cellar/jupyterlab/4.1.4/libexec/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ADP'"
     ]
    }
   ],
   "source": [
    "#Evaluate Fidelity\n",
    "# Identify columns to apply metrics\n",
    "continuous_columns = Orig_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_columns = Orig_data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Evaluate fidelity\n",
    "fidelity_results = evaluate_fidelity(Orig_data, synthetic_data, continuous_columns, categorical_columns)\n",
    "print(\"Fidelity Results:\", fidelity_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade6c2a-0f1e-4c13-bd3e-11a1d030e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate predictive performance\n",
    "predictive_performance_results = evaluate_predictive_performance(real_data, synthetic_data, 'target_column')\n",
    "print(\"Predictive Performance Results:\", predictive_performance_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd60442d-8c15-4262-82d9-cc44f3b895e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate informativeness\n",
    "informativeness_results = informativeness_test(real_data, synthetic_data)\n",
    "print(\"Informativeness Results:\", informativeness_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
